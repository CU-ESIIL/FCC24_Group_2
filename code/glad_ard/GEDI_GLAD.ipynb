{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import xarray as xr\n",
    "import glob\n",
    "from shapely.geometry import box\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge GEDI rh98 with GLAD ARD\n",
    "1. Open GLAD ARD annual mosaics built in `GLAD_ARD.ipynb` from 16 day-interval scenes of pre-normalized/harmonized landsat data from landsat4->landsat8 giving us temporal coverage of 1997-2023.\n",
    "2. Open GEDI rh98 tifs created by earth engine and dumped to drive here: https://code.earthengine.google.com/f60eee0697f93ad993a2ac91c505dfdf\n",
    "3. Reproject/Resample GLAD ARD to match GEDI (We try to avoid resampling/reprojecting the GEDI data if possible since geolocation/resampling errors may effect our estimates of model performance)\n",
    "\n",
    "Gedi tifs and `105W_39N_annual_median_composite.zarr` are uploaded to our team data-store [here](https://de.cyverse.org/data/ds/iplant/home/shared/earthlab/forest_carbon_codefest/Team_outputs/Team2?selectedOrder=asc&selectedOrderBy=name&selectedPage=0&selectedRowsPerPage=100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from team data store\n",
    "ard = xr.open_zarr(\"105W_39N_annual_median_composite.zarr\")\n",
    "aoi = box(*ard.rio.bounds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from team data store\n",
    "gedi_paths = glob.glob(\"../glad_ard/gedi/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build temporal mosaics simply by adding time coord and stacking along time axis\n",
    "gedi_dsets = []\n",
    "for f in gedi_paths:\n",
    "    dset = rioxarray.open_rasterio(f, chunks={\"x\": 1024,\"y\":1024})\n",
    "    dset = dset.expand_dims(\"time\")\n",
    "    year = int(f.split(\"_\")[-1].split(\".\")[0])\n",
    "    gedi_dsets.append(dset.assign_coords(time=(\"time\", [datetime(year=year, month=12, day=31)])))\n",
    "\n",
    "gedi = xr.concat(gedi_dsets, dim=\"time\")\n",
    "gedi = gedi.assign_coords(band=('band', ['rh98'])).to_dataset(\"band\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject ard to match gedi\n",
    "reprojected_dsets = []\n",
    "for year in tqdm(ard.time.data):\n",
    "    ard_reprojected = ard.sel(time=year).rio.reproject_match(gedi)\n",
    "    rr = ard_reprojected.expand_dims(\"time\").assign_coords(time=(\"time\", [year]))\n",
    "    reprojected_dsets.append(rr)\n",
    "reprojected_ard = xr.concat(reprojected_dsets, dim=\"time\")\n",
    "ard_reprojected = reprojected_ard['__xarray_dataarray_variable__'].to_dataset(\"band\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's add variable names for ard and merge ard and gedi together since they now have\n",
    "# matching bounds + geo transform\n",
    "array = ard_reprojected.to_array()\n",
    "ard_reprojected = array.assign_coords(variable=('variable', [\"Blue\",'Green','Red','NIR','SWIR1','SWIR2','Thermal','QF'])).to_dataset('variable')\n",
    "combined = xr.merge([ard_reprojected.chunk({'x': 1024, 'y': 1024, 'time': 1}), gedi.chunk({'x': 1024, 'y': 1024, 'time': 1})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out the dataset to avoid having to reproject/resample/rechunk/merge again.\n",
    "combined.chunk({'x': 1024, 'y': 1024, 'time': 1}).to_zarr(\"../glad_ard/ard_gedi.zarr\", mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Training Datasets with `ard_gedi.zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is also coppied to the team data store\n",
    "combined = xr.open_zarr(\"../glad_ard/ard_gedi.zarr/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish the independent and dependent variables\n",
    "covariates = [\"Blue\",'Green','Red','NIR','SWIR1','SWIR2','Thermal']\n",
    "targets = ['rh98']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build a training dataset we subset data to years where we know we\n",
    "# have gedi points\n",
    "years = [str(t) for t in range(2019, 2024)]\n",
    "X_subset = np.array(combined[covariates].isel(time=[-5,-4,-3,-2,-1]).to_array())\n",
    "Y_subset = np.array(combined[targets].isel(time=[-5,-4,-3,-2,-1]).to_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have some compute to work with and we use a pixel-based modeling\n",
    "# approach, let's flatten everything to get shape (n,p)\n",
    "y = Y_subset.reshape(len(targets), -1).T\n",
    "x = X_subset.reshape(len(covariates), -1).T\n",
    "\n",
    "# find examples where we have them as GEDI is sparse i.e. wherever data is\n",
    "# not nan\n",
    "idx = ~np.isnan(y).any(axis=1)\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "\n",
    "# clean both by dropping any nan in x (not y as we just did that above)\n",
    "valid_x = (~np.isnan(x).any(axis=1))\n",
    "x = x[valid_x]\n",
    "y = y[valid_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many cleaned examples?\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training datasets are ready! We have:\n",
    "- 1072397 examples\n",
    "- 7 independent variables from GLAD's ARD datasets (landsat bands)\n",
    "- 1 dependent variable.\n",
    "\n",
    "## Training\n",
    "We use the easiest method that is commonly used in the industry--boosted regression. The package we use here is xgboost, which also has nice support for gpu and distributed training on dask, etc. We do zero model development, feature engineering, feature selection, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should be splitting spatially! We are going to get optimistic results!\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)  # 80% training and 20% testing\n",
    "\n",
    "# nuance of xgboost are these DMatrices\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# lots to investigate here!\n",
    "params = {\n",
    "    'objective': 'reg:absoluteerror'\n",
    "}\n",
    "\n",
    "# more to investigate here\n",
    "num_round = 100  # the number of training iterations\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# run predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "# one simple evaluation metric as a sanity check\n",
    "print(mean_absolute_error(preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE of 3.21 not too bad, but beware of our splitting strategy! We should be estimating this through spatial cross validation for a more rigorous estimation of test error.\n",
    "\n",
    "Let's at least make a 1:1 plot and compare histograms between predictions and measured for y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test, bins=80, density=True, alpha=.4, label=\"measured\")\n",
    "plt.hist(preds, bins=80, color='r', density=True, alpha=.4, label='preds')\n",
    "plt.xlim(0,40)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note:\n",
    "- standard saturation issue for tall trees/things i.e. our predictions don't exhibit the same right skeweness that the measured values do.\n",
    "- minimum predictions are around 3m as expected with GEDI rh98\n",
    "- we at least match the bimodal nature of the distribution\n",
    "- there must be a few outliers since I needed to set the xlim to something reasonable, which means we could possible consider dropping anomalies in our measured rh98 values.\n",
    "- we are overpredicting rh98 at 15; we can't match the long right-skewed tail; and tend to miss lower predictions as they are shifted to higher values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "max_v = 35\n",
    "\n",
    "prediction = preds.flatten()\n",
    "measured = y_test.flatten()\n",
    "r2 = r2_score(prediction, measured)\n",
    "rmse = root_mean_squared_error(prediction, measured)\n",
    "bias = np.mean(measured - prediction)\n",
    "mae = mean_absolute_error(prediction, measured)\n",
    "\n",
    "n=10000\n",
    "idx = np.arange(0, len(prediction))\n",
    "np.random.shuffle(idx)\n",
    "prediction = prediction[idx[:n]]\n",
    "measured = measured[idx[:n]]\n",
    "\n",
    "# Evaluate KDE at each point\n",
    "kde = KernelDensity(bandwidth=2)\n",
    "kde.fit(np.vstack([prediction, measured]).T)\n",
    "xy = np.vstack([prediction, measured]).T\n",
    "density = np.exp(kde.score_samples(xy))\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(prediction, measured, c=density, cmap='viridis', alpha=.8, s=1)\n",
    "x = np.linspace(min(prediction), max_v, 100)\n",
    "plt.plot(x, x, color='red', linestyle='--')\n",
    "plt.xlim(0,max_v)\n",
    "plt.ylim(0,max_v)\n",
    "\n",
    "plt.text(\n",
    "    plt.xlim()[-1]*.05, \n",
    "    plt.ylim()[1] * .9, \n",
    "    f'R2: {r2:.2f} \\nRMSE: {rmse:.2f} \\nbias: {bias:.2f} \\nMAE: {mae:.2f}',\n",
    "    verticalalignment='top',\n",
    "    horizontalalignment='left',\n",
    "    fontsize=12\n",
    ")\n",
    "plt.title(\"Prediction vs. Measured (rh98)\")\n",
    "plt.xlabel(\"predicted rh98\")\n",
    "plt.ylabel(\"measured rh98\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate `rh98` over all GLAD-ARD 1997-2023\n",
    "We run the model over all available data now that things are trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariates = combined[covariates].to_array().chunk({'x': 256, 'y': 256, 'variable': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_predictions(block):\n",
    "    \"\"\"Define a function to apply over blocks\"\"\"\n",
    "    block = np.array(block)\n",
    "    xx = block.reshape(len(covariates), -1).T\n",
    "    xx = xgb.DMatrix(xx)\n",
    "    preds = bst.predict(xx)\n",
    "    ex = preds.reshape(1, -1).T.reshape(1, *block.shape[1:])\n",
    "    return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to each block to make estimates\n",
    "covariates = covariates.compute()\n",
    "predictions = covariates.map_blocks(run_predictions)\n",
    "preds = xr.DataArray(predictions[0], coords=combined[targets].coords)\n",
    "preds = preds.to_dataset(name='rh98-predictions')\n",
    "preds.chunk({\"time\":1, \"y\":1024,\"x\": 1024}).to_zarr(\"../data/predictions.zarr\", mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find `predictions.zarr` up on the team data-store.\n",
    "\n",
    "### Visualize Predictions over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geogif import gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xr.open_zarr(\"../data/predictions.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif(preds['rh98-predictions'], to=\"../data/predictions.gif\", fps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low res gif for the notebook (slice every 4th pixel in x and y)\n",
    "gif(preds['rh98-predictions'][:,::4,::4].compute(), fps=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codefest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
